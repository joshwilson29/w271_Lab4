---
title: "W271: Lab 4"
author: "Morris Burkhardt"
date: "April 22, 2018"
output:
  pdf_document: default
---


```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
library(lme4)
library(Hmisc)
library(plm)
```

# Description of the Lab

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for "per se" laws - where licenses can be revoked without a trial - and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.

**Exercises:**

1. Load the data. Provide a description of the basic structure of the dataset, as we have done in throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA.

```{r}
rm(list = ls())
```


```{r}
load('driving.RData')
head(data)
```

```{r}
summary(data)
```

```{r}
str(data)
```

The data set has the following 56 variables:
- year = integer; ranges from 1980 to 2004
- state = integer; ranges from 1 to 51
- sl55, sl65, sl70, sl75, slnone = decimal; fraction of year; all slxx variables sum up to 1
- seatbelt = integer; ranges from 0 to 2
- minage = decimal; weighted yearly minimum drinking age
- zerotol = decimal; fraction of year
- gdl = decimal; fraction of year
- bac10, bac08 = decimal; fraction of year
- perse = decimal; fraction of year
- totfat, nghtfat, wkndfat = integer
- totfatpvm, nghtfatpvm, wkndfatpvm = decimal
- statepop = integer
- totfatrte, nghtfatrte, wkndfatrte = decimal
- vehicmiles = decimal
- unem = decimal; ranges from 2.2 to 18
- per14_24 = decimal; ranges from 11.7 to 20.3
- sl70plus = decimal; fraction of year
- sbprim, sbsecon = dummy encoding of seatbelt variable
- d80, d81, ..., d04 = dummy encoding of year variable
- vehiclemilespc = decimal


Let us check if our panel is balanced:

```{r}
table(data$state)
```

Each of the 48 states has exactly 25 observations across time.


```{r}
sum(is.na(data))
```

There is no missing data in our data set.

```{r}
ggplot(data = data, aes(x = year, y = totfatrte, group = state)) + geom_line(alpha=0.5) + ggtitle("total fatality rate")

ggplot(data = data, aes(x = year, y = perc14_24, group = state)) + geom_line(alpha=0.5) + ggtitle("percentage of population between 14 - 24")

ggplot(data = data, aes(x = year, y = vehicmilespc, group = state)) + geom_line(alpha=0.5) + ggtitle("vehicle miles traveled per capita")

ggplot(data = data, aes(x = year, y = unem, group = state)) + geom_line(alpha=0.5) + ggtitle("unemployment rate")

ggplot(data = data, aes(x = year, y = seatbelt, group = state)) + ggtitle("seatbelt law (0 = none, 1 = primary offense, 2 = secondary offense)") + geom_jitter(width = 0.2, height = 0.2)

ggplot(data = data, aes(x = year, y = gdl, group = state)) + ggtitle("graduate drivers license law") + geom_jitter(width = 0.25, height = 0.1)

ggplot(data = data, aes(x = year, y = zerotol, group = state)) + ggtitle("zero tolerance law") + geom_jitter(width = 0.25, height = 0.1)

ggplot(data = data, aes(x = year, y = minage, group = state)) + ggtitle("minimum drinking age by state") + geom_jitter(width = 0.25, height = 0.1)

ggplot(data = data, aes(x = year, y = perse, group = state)) + ggtitle("administrative license revocation (per se law)") + geom_jitter(width = 0.25, height = 0.1)

# Create blood alcohol limit variable for law that was valid for the majority of the year.
data$baccombined = ifelse(round(data$bac10) > 0, 0.1, 0.08 * round(data$bac08))
ggplot(data = data, aes(x = year, y = baccombined, group = state)) + ggtitle("blood alcohol limit for majority of each year") + geom_jitter(width = 0.25, height = 0.005)

# Create speed limit variable, rounded to the nearest integer (55 = spead limit of  limit, 1 = limit of 0.08, 2 = limit of 0.1)
data$slcombined = ifelse(round(data$sl55) > 0, 55,
                          ifelse(round(data$sl65) > 0, 65,
                                 ifelse(round(data$sl70) > 0, 70,
                                        ifelse(round(data$sl75) > 0, 75, 99))))
ggplot(data = data, aes(x = year, y = slcombined, group = state)) + ggtitle("speed limit for majority of each year") + geom_jitter(width = 0.25, height = 1)
```

The first seatbelt laws were implemented in 1985 and by 1995, all but one state had either a primary or secondary offence seatbelt law in place. The distribution remained the same until 2004 (still one state had not implemented a seatbelt law).

Up until the mid 90s, no state had a graduate drivers licence law in place. States began implementing the law in the mid 90s and by 2004, about 80% of all states had implemented a graduate drivers licence law.

In the early 80s, no state had a zero tolerance law in place. The first states that implemented a zero tolerance law did so in the mid 80s. The number of states with zero tolerance laws gradually increased and by the 1998, all states had a zero tolerance law in place. 

The minimum drinking age was between 18 and 21 until the mid 80s, when all states implemented a minimum drinking age of 21.

Per se laws were introduced in the early 80 and the amount of states that had per se laws in plays increased gradually. By 2004, almost all states had per se laws implemented.

Blood alcohol limit was either 0 or 0.1 in the early 80s. In the mid 80s, they were reduced to 0.08 and more states that had zero as limit set it to 0.08. In 2004 almost all states had a blood alcohol limit of 0.08.

Speed limits were increased in time!


Maybe better to only display those that are being used / those that are per capita...

```{r, fig.height = 7}
hist(data[c('sl55','sl65','sl70','sl75','slnone','seatbelt','minage','zerotol','gdl','bac10','bac08','perse','sl70plus')])

hist(data[c('totfat','nghtfat','wkndfat','totfatpvm','nghtfatpvm','wkndfatpvm','statepop','totfatrte','nghtfatrte','wkndfatrte','vehicmiles','unem','perc14_24','vehicmilespc')])
```



```{r, fig.height=9}
dotplot(reorder(state, totfatrte) ~ totfatrte, data, groups = state, 
        ylab = 'State', xlab = 'Total fatalities rate per 100000', 
        type = c('p', 'a'), auto.key = list(columns = 3, lines = TRUE))
```

2. How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a very simple regression model of totfatrte on dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

The totfarte variable holds the total fatalities per 100,000 population.

```{r}
df_mean_totfarte = data.frame(aggregate(data$totfatrte, by = list(data$year), mean))
colnames(df_mean_totfarte) <- c("year", "mean_totfarte")
df_mean_totfarte
```

We specify 1980 as the base year and fit the following linear regression model:

\begin{equation*}
\begin{aligned}
totfatrte =& \beta_0 + \beta_1 \cdot d_{81} + \beta_2 \cdot d_{82} + \beta_3 \cdot d_{83} + \beta_4 \cdot d_{84} + \beta_5 \cdot d_{85} + \beta_6 \cdot d_{86} + \beta_7 \cdot d_{87} + \beta_8 \cdot d_{88} + \beta_9 \cdot d_{89} \\
      & + \beta_{10} \cdot d_{90} + \beta_{11} \cdot d_{91} + \beta_{12} \cdot d_{92} + \beta_{13} \cdot d_{93} + \beta_{14} \cdot d_{94} + \beta_{15} \cdot d_{95} + \beta_{16} \cdot d_{96} + \beta_{17} \cdot d_{97}\\
      &  + \beta_{18} \cdot d_{98} + \beta_{19} \cdot d_{99} + \beta_{20} \cdot d_{00} + \beta_{21} \cdot d_{01} + \beta_{22} \cdot d_{02} + \beta_{23} \cdot d_{03} + \beta_{24} \cdot d_{04} 
\end{aligned}
\end{equation*}

```{r}
lm1 = lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 +
         d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04, 
         data = data)
summary(lm1)
```

All estimators but one (d81) are highly statistically significant. 

This model explains exactly what was calculated in the aggregation above. By adding a single dummy variable to the intercept, we receive the mean totfatrte value for the respective year. Below, we show this for three randomly selected years.

```{r}
set.seed(999)
results = data.frame()
for (i in 1:25) {
  mean_value_lm1 = ifelse(i==1, lm1$coefficients[1], lm1$coefficients[1] + lm1$coefficients[i])
  year = df_mean_totfarte$year[i]
  mean_value_agg = df_mean_totfarte$mean_totfarte[i]
  difference = round(mean_value_lm1 - mean_value_agg,5)
  results = rbind(results, cbind(year, mean_value_agg, mean_value_lm1, difference))
}
sample_n(results, 3)
```

Let us take a look at some diagnostic plots.

```{r, fig.height = 2.5, fig.width=12}
par(mfrow = c(1, 3))
plot(lm1, which = c(1,2,3))
```

This model is - obviously - missing a lot of explanatory variables...


3. Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

Like in question 2, we are fitting a pooled OLS linear regression model, but this time we are including more explanatory variables than just the dummy variables for the years. We are including:
- bac08 and bac10, which indicate the fraction of the year for which a blood alcohol limit of 0.08 and 0.1 respectively was in place, - perse, wich indicates the fraction of the year for which a per se law was in place,
- sbprim and sbsecond, which are dummy variables that indicate respectively, whether a primary or secondary seatbelt law was in place,
- sl70plus, which indicates the fraction of the year for which a speed limit of >= 70 was in place,
- gdl, which indicates the fraction of the year for which a graduated drivers license law was in place,
- perc14_24, which holds the percentage of the population aged 14 through 24,
- unem, which holds the unemployed rate
- vehicmilespc, which holds the vehicle miles traveled per capita

Most of our newly added variables are either dummy variables (sbprim, sbsecond) or fractions of a year (bac08, bac10, perse, sl70plus, gdl), which mainly hold either 0 or 1. Naturally, the distributions for these variables all have extreme peaks at 0 and 1.

We were debating on whether or not to bin the 'fraction of year' variables to 0 and 1. We however believe that the information loss outweighs the gain in comprehensibility/simplicity.

Discussion of variable transformation for the remaining variables:

We will draw histograms of the original variable plus possible transformation. We will furthermore perform Shapiro Wilk Tests on the distribution before and after transformation to check if the transformation improved the distribution, in terms of being closer to a normal distribution. With the Shapiro Wilk Test, we test against the null hypothesis, that the variable stems from a normal distribution.

1. totfatrte variable:

```{r, fig.height = 3, fig.width = 12}
par(mfrow = c(1, 2))
hist(data$totfatrte, breaks = 20)
hist(log(data$totfatrte), breaks = 20)
```

```{r}
shapiro.test(data$totfatrte)$p.value
shapiro.test(log(data$totfatrte))$p.value
```

==> Log transform improves p-value, but still far from not significant!

2. perc14_24 variable:

```{r, fig.height = 3, fig.width = 12}
par(mfrow = c(1, 2))
hist(data$perc14_24, breaks = 20)
hist(log(data$perc14_24), breaks = 20)
```

```{r}
shapiro.test(data$perc14_24)$p.value
shapiro.test(log(data$perc14_24))$p.value
```

==> No significant improvement! p-value still highly significant.

3. unem variable:

```{r, fig.height = 3, fig.width = 12}
par(mfrow = c(1, 2))
hist(data$unem, breaks = 20)
hist(log(data$unem), breaks = 20)
```

```{r}
shapiro.test(data$unem)$p.value
shapiro.test(log(data$unem))$p.value
```

==> Significant improvement! Log transformed variable is likely to be normally distributed.

4. vehicmilespc variable 

```{r, fig.height = 3, fig.width = 12}
par(mfrow = c(1, 2))
hist(data$vehicmilespc, breaks = 20)
hist(log(data$vehicmilespc), breaks = 20)
```

```{r}
shapiro.test(data$vehicmilespc)$p.value
shapiro.test(log(data$vehicmilespc))$p.value
```

==> p-value much higher, but still far away from being non-significant!

Conclusion:

The totfatrte variable has a right skew. A log transformation however was not able to significantly improve the p-value of the Shapiro Wilk Test, which is why we decided to not transform this variable.

The distribution of the perc14_24 variable has an abnormal peak around 14, but looks well balanced otherwise. A log transformation did not improve the distribution significantly. The Shapiro Wilk Test for the log transformed variable still shows high statistical significance.

The distribution of the unem variable has a strong right skew which can be beautifully removed using a log transformation. The resulting log(unem) variable closely resembles a normal distribution. The resemblance with the normal distribution is confirmed by the Shapiro Wilk Test. The p-value is close to 0.3 and hence we fail to reject the null hypothesis, which means that it is likely that log(unem) stems from a normal distribution.

The distribution of the vehicmilespc variable is quite a bit leptokurtic, but looks balanced otherwise. While a log transformation significantly increased the p-value in the Shapiro Wilk Test, the p-value is still far away from being not significant. We therefore do not perform a transformation on the vehicmilespc variable.

We are therefore specifying the following model:

\begin{equation*}
\begin{aligned}
totfatrte =& \beta_0 + \beta_1 \cdot d_{81} + \beta_2 \cdot d_{82} + \beta_3 \cdot d_{83} + \beta_4 \cdot d_{84} + \beta_5 \cdot d_{85} + \beta_6 \cdot d_{86} + \beta_7 \cdot d_{87} + \beta_8 \cdot d_{88} + \beta_9 \cdot d_{89} \\
      & + \beta_{10} \cdot d_{90} + \beta_{11} \cdot d_{91} + \beta_{12} \cdot d_{92} + \beta_{13} \cdot d_{93} + \beta_{14} \cdot d_{94} + \beta_{15} \cdot d_{95} + \beta_{16} \cdot d_{96} + \beta_{17} \cdot d_{97}\\
      &  + \beta_{18} \cdot d_{98} + \beta_{19} \cdot d_{99} + \beta_{20} \cdot d_{00} + \beta_{21} \cdot d_{01} + \beta_{22} \cdot d_{02} + \beta_{23} \cdot d_{03} + \beta_{24} \cdot d_{04} + \beta_{25} \cdot bac08\\
      & + \beta_{26} \cdot bac10 + \beta_{27} \cdot perse + \beta_{28} \cdot sbprim + \beta_{29} \cdot sbsecon + \beta_{30} \cdot gdl + \beta_{31} \cdot perc_{14\_24} + \\ 
      & + \beta_{32} \cdot \log(unem) + \beta_{33} \cdot vehicmilespc
\end{aligned}
\end{equation*}

```{r}
lm2 = lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
         d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
         bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
         vehicmilespc, data = data)
summary(lm2)
```


4. Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

By specifiying a Fixed Effects Model, we account for the time invariant unobserved hetereogenity (the whole point of doing fixed effect model!)

```{r}
fe1 = plm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
          d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
          bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
          vehicmilespc, data = data, model = 'within', index = c('state', 'year'))
summary(fe1)
```


lm model output of the four variables under study.

bac08        -2.626e+00  5.454e-01  -4.815 1.67e-06 ***
bac10        -1.402e+00  4.020e-01  -3.487 0.000506 ***
perse        -4.423e-01  3.019e-01  -1.465 0.143122    
sbprim       -2.954e-01  5.002e-01  -0.591 0.554853    

fe model output of the four variables under study.

bac08        -1.3185e+00  3.9535e-01  -3.3351 0.0008808 ***
bac10        -9.6089e-01  2.6936e-01  -3.5673 0.0003759 ***
perse        -1.2151e+00  2.3232e-01  -5.2300 2.022e-07 ***
sbprim       -1.1696e+00  3.4259e-01  -3.4140 0.0006632 ***


5. Would you perfer to use a random effects model instead of the fixed effects model you build in *Exercise 4*? Why? Why not?

We believe that the explanatory variables are correlated with the unobserved/fixed effect ==> Random Effects Model would not be good!

ToDo!More discussion!

Let us see what it looks like...

```{r}
re1 = plm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
          d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
          bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
          vehicmilespc, data = data, model = 'random', index = c('state', 'year'))
summary(re1)
```


6. Suppose that *vehicmilespc*, the number of miles driven per capita, increases by 1,000. Using the FE estimates, what is the estimated effect on totfatrte? Be sure to interpret the estimate as if explaining to a layperson.



7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the coefficient estimates and their standard errors?

Consequence:


Hausman Test!!!









