---
title: "W271: Lab 4"
author: "Morris Burkhardt/group"
date: "April 22, 2018"
geometry: margin = 1.3 cm
fontsize: 10 pt
output:
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

pkg <- c('ggplot2', 'dplyr', 'lme4', 'Hmisc', 'plm', 'gridExtra')
invisible(lapply(pkg, require, character.only = T))
```

# Question 1.

First, we load the data and take a look at the head, tail, summary and str of the data to get a general idea of the data set (partially not displayed here).

```{r}
load('driving.RData')
head(data, 2)[1:14]; #tail(data); #summary(data); #str(data)
```


Our data set is a longitudinal data set. It consists of the data of the 48 continental states over the time period from 1980 to 2004. 

The data set has the following 56 variables:

- year = year of observation; integer; ranges from 1980 to 2004

- state = state; integer; ranges from 1 to 51 (missing 2, 9 and 12: Alaska, Hawai and D.C.)

- sl55, sl65, sl70, sl75, slnone = speed limit 55, 65, 70, 75 and non respectively; decimal; fraction of year; all these five variables sum up to 1

- seatbelt = indicates what type of seatbelt law was inplace: no seatbelt law (=0), primary seatbelt law (=1), secondary seatbelt law (=2); integer; ranges from 0 to 2

- minage = minimum drinking age; decimal; weighted yearly minimum drinking age; ranges from 18 to 21

- zerotol = zero tolerance law; decimal; fraction of year for which a zero tolerance was in place 

- gdl = graduated driver license law; decimal; fraction of year for which a gdl law was in place

- bac10, bac08 = blood alcohol limit of 0.1, 0.08 respectively; decimal; fraction of year for which each limit was in place

- perse = per se law; decimal; fraction of year for which a per se law was in place

- totfatpvm, nghtfatpvm, wkndfatpvm = total, nighttime, weekend fatalities, respectively; integer

- totfat, nghtfat, wkndfat, statepop = state population; integer

- totfatrte, nghtfatrte, wkndfatrte = total, nighttime and weekend fatalities per 100,000 population, respectively; decimal

- vehicmiles = billion vehicle miles traveled; decimal

- unem = unemployment rate in percent; decimal; ranges from 2.2 to 18

- per14_24 = percentage of population aged 14 through 24; decimal; ranges from 11.7 to 20.3

- sl70plus = sum of the sl70, sl75 and slnone variables; decimal; fraction of year

- sbprim, sbsecon = primary, secondary seatbelt law respectively; dummy encoding of seatbelt variable

- d80, d81, ..., d04 = year 1980, year 1981,... year 2004 respectively; dummy encoding of year variable

- vehiclemilespc = vehicle miles per capita; decimal

Let us check if our panel is balanced:

```{r}
min(table(data$state)); max(table(data$state))
```

Each of the 48 states has exactly 25 observations across time, which means that our panel is balanced. Next, let us check for missing data.

```{r}
sum(is.na(data))
```

There is no missing data in our data set. Next, we will display the variables we will be using in our model graphically. For the totfartre, perc14_24, vehicmilespc and unem variable, we plot a line for every state (colorized) over the course of all the years and for the seatbelt, gdl, zerotol, perse, bacXX and slXX variables we create scatter plots (with jitter) that display the distribution accross the different states for every year. To better display the data, we created a combined blood alcohol limit and speed limit variable.

```{r, fig.height = 16, fig.width = 12}
plot1 = ggplot(data = data, aes(x = year, y = totfatrte, colour = as.factor(state))) + 
  geom_line(alpha=0.5, show.legend = F) + ggtitle("Total Fatality Rate (per 100,000 population)") +
  theme(plot.title = element_text(hjust = 0.5)) +  ylab("Total Fatality Rate")
plot2 = ggplot(data = data, aes(x = year, y = perc14_24, colour = as.factor(state))) + 
  geom_line(alpha=0.5,show.legend = F) + ggtitle("Percentage of Population between Ages 14 and 24") + 
  theme(plot.title = element_text(hjust = 0.5)) + ylab("Percent of Population")
plot3 = ggplot(data = data, aes(x = year, y = vehicmilespc, colour = as.factor(state))) + 
  geom_line(alpha=0.5, show.legend = F) + ggtitle("Vehicle Miles Traveled Per Capita") +
  theme(plot.title = element_text(hjust = 0.5)) + ylab("Miles Per Capita")
plot4 = ggplot(data = data, aes(x = year, y = unem, colour = as.factor(state))) + 
  geom_line(alpha=0.5, show.legend = F) + ggtitle("Unemployment Rate") + 
  theme(plot.title = element_text(hjust = 0.5)) + ylab("Percent Unemployed")
plot5 = ggplot(data = data, aes(x = year, y = seatbelt, group = state)) + 
  ggtitle("Seatbelt Laws by Year") + 
  geom_jitter(width = 0.2, height = 0.2, alpha = 0.3) + theme(plot.title = element_text(hjust = 0.5)) +
  ylab("Law Type: \n 0 = none, 1 = primary offense,\n 2 = secondary offense)")
plot6 = ggplot(data = data, aes(x = year, y = gdl, group = state)) + 
  ggtitle("Graduate Drivers License Law") + geom_jitter(width = 0.25, height = 0.1, alpha = 0.3) +
  theme(plot.title = element_text(hjust = 0.5)) + ylab("% of Year with Law")
plot7 = ggplot(data = data, aes(x = year, y = zerotol, group = state)) + 
  ggtitle("Zero Tolerance Law") + geom_jitter(width = 0.25, height = 0.1, alpha = 0.3) +
  theme(plot.title = element_text(hjust = 0.5)) + ylab("% of Year with Law")
plot8 = ggplot(data = data, aes(x = year, y = perse, gcolour = as.factor(state))) + 
  ggtitle("Administrative License Revocation (per se law)") + 
  geom_jitter(width = 0.25, height = 0.1, alpha = 0.3) + 
  theme(plot.title = element_text(hjust = 0.5)) + ylab("% of Year with Law")
# Create combined blood alcohol limit variable for law that was valid for the majority of the year.
data$baccombined = ifelse(round(data$bac10) > 0, 0.1, 0.08 * round(data$bac08))
plot9 = ggplot(data = data, aes(x = year, y = baccombined, group = state)) + 
  ggtitle("BAC for Majority of Each Year") + 
  geom_jitter(width = 0.25, height = 0.005, alpha = 0.3) + 
  theme(plot.title = element_text(hjust = 0.5)) + ylab("BAC Limit by Law")
# Create combined speed limit variable.
data$slcombined = ifelse(round(data$sl55) > 0, 55, ifelse(round(data$sl65) > 0, 65,
                         ifelse(round(data$sl70) > 0, 70, ifelse(round(data$sl75) > 0, 75, 99))))
plot10 = ggplot(data = data, aes(x = year, y = slcombined, group = state)) + 
  ggtitle("Speed Limit for Majority of Each Year") + geom_jitter(width = 0.25, height = 1, alpha = 0.3) +
  theme(plot.title = element_text(hjust = 0.5)) + ylab("Speed Limit")
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, 
             plot7, plot8, plot9, plot10, ncol=2)
```

For every state, the total fatality rate sank between 1980 and about 1995. Especially in the early eighties there seems to be a steep fall off in the fatality rate. It looks like after 1995 the total fatality rate remained roughly the same for every state. 

For 47 out of the 48 continental states under study, the percentage of population between 14 and 24 sank rapidly (from roughly 18% to roughly 14%) between 1980 and and 1990. After that the percentage remained roughly the same.

The vehicle miles traveled per capita increased continuously over the years from 1980 to 2004. There is a strong positive linear trend for each of the states.

The unemployment rate briefly went up in the early eighties and then immediately decreased until the late eighties. In the early nineties the unemployment rate once again increased briefly (on a lower level than before), sank again until around 2000, where it reached its lowest point, before it slightly increase again in the early 00s. It looks like it started decreasing again around 2003.

The first seatbelt laws were implemented in 1985 and by 1995, all but one state had either a primary or secondary offence seatbelt law in place. The distribution remained the same until 2004 (still one state had not implemented a seatbelt law).

Up until the mid 90s, no state had a graduate drivers licence law in place. States began implementing the law in the mid 90s and by 2004, about 80% of all states had implemented a graduate drivers licence law.

In the early 80s, no state had a zero tolerance law in place. The first states that implemented a zero tolerance law did so in the mid 80s. The number of states with zero tolerance laws gradually increased and by the 1998, all states had a zero tolerance law in place. 

The minimum drinking age was between 18 and 21 until the mid 80s, when all states implemented a minimum drinking age of 21.

Per se laws were introduced in the early 80 and the amount of states that had per se laws in place increased gradually. By 2004, almost all states had per se laws implemented.

Blood alcohol limit was either 0 or 0.1 in the early 80s. In the mid 80s, they were reduced to 0.08 and more states that had zero as limit set it to 0.08. In 2004 almost all states had a blood alcohol limit of 0.08.

For all states, the speed limits were increased with time. In 1980, all states had a speed limit of 55, wherease by 2004 all states had a speed limit of at least 65, with a substantial number of states having speed limits above. 

Next, we take a look at histograms for the non-dummy and non-'fraction of the year' variables that we will be using in our models.

```{r, fig.height = 4, fig.width = 8}
hist(data[c('totfatrte','unem','perc14_24','vehicmilespc')])
```

All of these variables show some degree of positive skew and we will be discussing possible transformations in question 3.

As we can see in the boxplot below, there are huge differences between the distributions of the total fatality rate between different states over the course of the 25 years - in terms of median and variation. The median total fatality rate across states varies between somewhere around 10 to somewhere around 30. Some of the states have very little variation (about 5 points), while others vary widely (about 20 points). A lot of the differences in variation can probably be attributed to the (very unequal) size of the the states. The boxplot to the very right for instance shows the totfatrte distribution for the state of Wyoming which had (and still has) the lowest population of all states (about 500,000 in 2004). Wyoming also has the highest outlier with a total fatality rate of about 50 per 100,000 population.

```{r, fig.width = 8, fig.height = 4}
ggplot(data,aes(x=state, y=totfatrte, group = state)) + geom_boxplot(fill='#00bfff') + 
  guides(fill=FALSE) + ggtitle("Boxplot of totfatrte for each state") 
```

# Question 2:

The totfarte variable holds the total fatalities per 100,000 population. The yearly averages are displayed in the table below.

```{r}
df_mean_totfatrte = data.frame(aggregate(data$totfatrte, by = list(data$year), mean))
colnames(df_mean_totfatrte) <- c("year", "mean_totfatrte")
df_mean_totfatrte
```

We specify 1980 as the base year and fit the following linear regression model:

\begin{equation*}
\begin{aligned}
totfatrte =& \beta_0 + \beta_1 \cdot d_{81} + \beta_2 \cdot d_{82} + \beta_3 \cdot d_{83} + \beta_4 \cdot d_{84} + \beta_5 \cdot d_{85} + \beta_6 \cdot d_{86} + \beta_7 \cdot d_{87} + \beta_8 \cdot d_{88} + \beta_9 \cdot d_{89} \\
      & + \beta_{10} \cdot d_{90} + \beta_{11} \cdot d_{91} + \beta_{12} \cdot d_{92} + \beta_{13} \cdot d_{93} + \beta_{14} \cdot d_{94} + \beta_{15} \cdot d_{95} + \beta_{16} \cdot d_{96} + \beta_{17} \cdot d_{97}\\
      &  + \beta_{18} \cdot d_{98} + \beta_{19} \cdot d_{99} + \beta_{20} \cdot d_{00} + \beta_{21} \cdot d_{01} + \beta_{22} \cdot d_{02} + \beta_{23} \cdot d_{03} + \beta_{24} \cdot d_{04} 
\end{aligned}
\end{equation*}

```{r}
lm1 = lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 +
         d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04, 
         data = data)
summary(lm1)
```

All parameter estimates except the one for 1981 are highly statistically significant. 

The model above explains the average difference in the yearly fatalities as compared to the baseline year of 1980; this perspective pools the states by year. Essentially, this model construction allows for varying intercepts for each year, meaning we can model differences in the distributions year-over-year. Every subsequent year after 1980 shows an average decrease in our dependent variables (despite an increase in vehicle miles per year per capita as can be seen in the EDA section), with later years showing greater decreases. The improvement however did not occur 'continuously' (year by year), but rather in two steps. The first improvement took place between 1980 and 1983 (decrease of roughly 5.3) and the second improvement took place between 1990 and 1992 (decrease of around 2.3). During the other time periods, the total traffic fatalaties remained roughly constant every year (with just some 'natural' variation). As there are no other explanatory variables in this model, the year variables are capturing variability that might come from new laws being enacted, improvements in car safety technology, and other factors related to the rise and fall of total vehicular fatalities. From the individual t-tests, nearly all of the years are significantly different from 1980, with the exception of 1981. Therefore, it is unsuprising to see that the F test is also significant. In order to determine whether this is an appropriate model for our task, we would want to check whether the variance in error changes over time. Regardless, this model is likely to suffer from omitted variable bias, as we have very few variables that are time-varying.

This model explains exactly what was calculated in the aggregation above. If we add any of the above estimated dummy parameters to the intercept, we receive the mean totfatrte value for the respective year. Below, we show this for three randomly selected years.

```{r}
set.seed(999); results = data.frame()
for (i in 1:25) {
  mean_value_lm1 = ifelse(i==1, lm1$coefficients[1], lm1$coefficients[1] + lm1$coefficients[i])
  year = df_mean_totfatrte$year[i]
  mean_value_agg = df_mean_totfatrte$mean_totfarte[i]
  difference = round(mean_value_lm1 - mean_value_agg,5)
  results = rbind(results, cbind(year, mean_value_agg, mean_value_lm1, difference))
}
sample_n(results, 3)
```

We looked at the residual plot for this model and saw that the residuals are not well behaved. This model is not taking the nested dependency (across states) within the data into account. Furthermore, we strongly believe that several important explanatory variables are missing from the model.

# Question 3:

Like in question 2, we are fitting a pooled OLS linear regression model, but this time we are including more explanatory variables than just the dummy variables for the years. We are including bac08 and bac10, perse, sbprim and sbsecond, sl70plus, gdl, perc14_24, unem and vehicmilespc.

Most of our newly added variables are either dummy variables (sbprim, sbsecond) or indicate fractions of a year (bac08, bac10, perse, sl70plus, gdl), which mainly hold either 0 or 1. Naturally, the distributions for these variables all have extreme peaks at 0 and 1.

We were debating on whether or not to bin the 'fraction of year' variables to 0 and 1. We however believe that the information loss outweighs the gain in comprehensibility/simplicity, which is why we decided to leave the variables as they are.

For the other variables (totfatrte, perc14_24, unem and vehicmilespc) we looked at the histograms (see EDA section) and based on that tried out different transformations. We furthermore performed Shapiro Wilk Tests on the distributions before and after transformation to check if the transformation improved the distribution, in terms of being closer to a normal distribution. The Shapiro Wilk Test tests against the null hypothesis, that the variable stems from a normal distribution.

The totfatrte variable has a right skew. We were not able to find a simple transformation that significantly improved the p-value of the Shapiro Wilk Test, which is why we decided to not transform this variable. The distribution of the perc14_24 variable has an abnormal peak around 14, but looks well balanced otherwise. Different transormations did not improve the distribution significantly. For every transformation (including log), the Shapiro Wilk Test continued to show high statistical significance. The distribution of the unem variable has a strong right skew which can be beautifully removed using a log transformation. The resulting log(unem) variable closely resembles a normal distribution and this is also confirmed by the Shapiro Wilk Test. The p-value is close to 0.3 and hence we fail to reject the null hypothesis, which means that it is likely that log(unem) stems from a normal distribution (see analysis below). The distribution of the vehicmilespc variable is quite a bit leptokurtic, but balanced otherwise. While a log transformation significantly increased the p-value in the Shapiro Wilk Test, the p-value is still far away from being not significant. We therefore decided to not perform a transformation on the vehicmilespc variable.

```{r, fig.height = 2.5, fig.width = 12}
par(mfrow = c(1, 2))
hist(data$unem, breaks = 20)
hist(log(data$unem), breaks = 20)
```

```{r}
shapiro.test(data$unem)$p.value
shapiro.test(log(data$unem))$p.value
```


We are therefore specifying the following model:

\begin{equation*}
\begin{aligned}
totfatrte =& \beta_0 + \beta_1 \cdot d_{81} + \beta_2 \cdot d_{82} + \beta_3 \cdot d_{83} + \beta_4 \cdot d_{84} + \beta_5 \cdot d_{85} + \beta_6 \cdot d_{86} + \beta_7 \cdot d_{87} + \beta_8 \cdot d_{88} + \beta_9 \cdot d_{89} \\
      & + \beta_{10} \cdot d_{90} + \beta_{11} \cdot d_{91} + \beta_{12} \cdot d_{92} + \beta_{13} \cdot d_{93} + \beta_{14} \cdot d_{94} + \beta_{15} \cdot d_{95} + \beta_{16} \cdot d_{96} + \beta_{17} \cdot d_{97}\\
      &  + \beta_{18} \cdot d_{98} + \beta_{19} \cdot d_{99} + \beta_{20} \cdot d_{00} + \beta_{21} \cdot d_{01} + \beta_{22} \cdot d_{02} + \beta_{23} \cdot d_{03} + \beta_{24} \cdot d_{04} + \beta_{25} \cdot bac08\\
      & + \beta_{26} \cdot bac10 + \beta_{27} \cdot perse + \beta_{28} \cdot sbprim + \beta_{29} \cdot sbsecon + \beta_{30} \cdot gdl + \beta_{31} \cdot perc_{14\_24} + \\ 
      & + \beta_{32} \cdot \log(unem) + \beta_{33} \cdot vehicmilespc
\end{aligned}
\end{equation*}

```{r}
lm2 = lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
         d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
         bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
         vehicmilespc, data = data)
summary(lm2)
```

The estimated model parameter is roughly -2.6 for the bac08 variable and roughly -1.4 for bac10 variable. This means the model suggests that (ceteris paribus), when a blood alcohol limit of 0.08 is introduced, states can expect to lower their total traffic fatalities rate per 100,000 by about 2.6 per year. If (ceteris paribus) a blood alcohol limit of 0.1 is introduced, states can expect to lower their total traffic fatalities rate per 100,000 by about 1.4 per year. This result is surprising and certainly counter intuitive. 
By looking at our graphical representation of the combined bac08 and bac10 variable in the EDA section, this result can however be explained: While only a fraction of the states introduced a blood alcohol limit of 0.1 throughout the period of our study, it looks like (almost) all states implemented a 0.08 blood alcohol limit in 2004 (possibly a federal law) - and quite a few states even did so before. This means:

- When states implemented a 0.1 limit, they always RAISED the blood alcohol limit (from either 0 or 0.08 to 0.1) and we would expect a decrease in the traffic fatalities.

- However, when states implemented a 0.08 limit, they either RAISED the blood alcohol limit from 0 to 0.08 (we would epect a decrease in the traffic fatalities) or they LOWERED it from a previously existing 0.1 blood alcohol limit (we would expect an increase in the traffic fatalities). 
As the model doesn't account for the historic state in terms of blood alcohol limit, the parameters are 'skewed'. That is especially true for the 0.08 blood alcohol limit, as the change in limit can be in either direction, whereas for the 0.1 limit it always went in the same direction (and is only of different magnitude).

According to our model, an implementation of per se laws slightly lowers the fatality rate. For a full-year implementation of a per se law we would expect a decrease in the total fatalities rate of about 0.44. This result however is not statistically significant and therefore should not be taken at face value.

According to our model, the introduction of a primary seat belt law lowers the fatality rate. For a full-year implementation of a primary seat belt law, we would expect a decrease in the total fatalities rate of about 0.3. Again, we have the problem that certain states might have switched from a secondary to a primary seatbelt law (rather than newly implement a seat belt law), which would skew this parameter. The result however is far away from statistical significane and therefore should not be taken at face value.

# Question 4:

We reestimate the model from Question 3 using a Fixed Effects Model at the state level. By specifiying a Fixed Effects Model, we account for the time invariant unobserved hetereogenity.

```{r}
fe1 = plm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
          d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
          bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
          vehicmilespc, data = data, model = 'within', index = c('state'))
summary(fe1)
```

From a glance at the model outputs, we can see that directionally all of the estimates are consistent across both models. The pooled OLS model has higher estimates for all of the variables except bac10. The standard errors for the fixed effect models are uniformly lower than all of the fixed effects for the OLS model, potentially indicating higher precision in the fixed effect model. All of the variables in the fixed effect model are statistically significant at the 0.05 level, whereas only the two bac variables are statistically significant in the pooled model. We believe that the independent variables vary with the unobserved effects, making the pooled OLS model inconsistent because this violates an assumption of the model. The unobserved effect of state is likely correlated with the other explanatory variables: For example, Utah is a state with tighter laws on alcohol due to the predominantly Mormon population. Further, the Hausman test (see below) also supports that the fixed effects model is the better choice because we reject the null hypothesis that the unobserved effect does not covary with explanatory variables.

```{r}
phtest(fe1, lm2)
```

The fixed effect model has various assumptions that can be found on pg 457 and 458 of the Wooldridge assigned reading. The important assumptions to note for this modeling exercise are that each explanatory variable changes over time (and that there are no perfect linear relationships between the explanatory variables) and that the expected value of the idiosyncratic error is 0 given the explanatory variables and the unobserved effects (strict exogeneity assumption). In the case of the comparison between the FE model and pooled OLS, the variables changing over time is the most debilitating argument against a pooled OLS.

# Question 5:

To determine wheter or not a random effects model should be used instead of the fixed effects model, we first conduct a Hausman Test, with null hypothesis that the random effects assumptions are correct.

```{r}
re1 = plm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + 
          d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04 + 
          bac08 + bac10 + perse + sbprim + sbsecon + gdl + perc14_24 + log(unem) + 
          vehicmilespc, data = data, model = 'random', index = c('state'))
phtest(fe1, re1)
```

We reject the null hypothesis with a highly statistically significant p-value. The Hausman Test therefore suggests to use a Fixed Effects model. 

Since furthermore none of our explanatory variables are constant over time (which can't be modeled using fixed effects) and we observe quite a bit of variability in the estimated fixed effects for our `fe1` model (see histogram of the fixed effect accross the states below), we believe that the Fixed Effects Model is the better model for our scenario.

```{r fig.height=3, fig.width = 8}
hist(fixef(fe1), breaks = 20, c='navy', xlab='Fixed effect size',
     main='Distribution of Fixed effect Size')
```

# Question 6:

According to our Fixed Effects model we can state the following:

If - on average - people were to drive 1,000 miles more per year, we would expect the rate of total traffic fatalaties per 100,000 people to increase by roughly 1 (0.96 according to model). Let us explain this with an example. In 2004, the United States had a popluation of roughly 300 million people. If - on average - people had driven 1,000 miles more in the year of 2004 than they actually did, we would expect about 2,880 additional traffic fatalities.

# Question 7:

The fixed effects model assumes that the idiosyncratic errors are uncorrelated. A violation of this assumption will result in the relationship between the predictor and outcome variables being mischaracterized. Further, the estimated variance will be biased, resulting in an invalid estimate for the standard error as well. We conducted a Breusch-Pagan F-test to determine if our fixed effects model shows any serial correlation (see below). Unfortunately, we reject the null hypothesis: Our model displays symptoms of serially correlated errors. If there is heteroskedasticity or serial correlation in the idiosyncratic errors, the coefficients are not necessarily biased, but the standard errors are likely to be overstated (in case of heteroskedasticity) or understated (in case of serial correlation). The first four assumptions ([W2016] pg 457) of the FE model get us unbiased estimators; it is the final 3 assumptions that get us a Best Linear Unbiased Estimator.

```{r}
pbgtest(fe1, type = "F")
```

